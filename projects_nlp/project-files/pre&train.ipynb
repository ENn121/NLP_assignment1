{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n"
     ]
    }
   ],
   "source": [
    "#pre-process train_data and label\n",
    "import json\n",
    "f = open('/Users/enn/Desktop/nlp/project1/project-files/train_total.json')\n",
    "train_data = json.load(f)\n",
    "\n",
    "train = []\n",
    "y_train = []\n",
    "\n",
    "for key, value in train_data.items():\n",
    "    train.append(value['text'])\n",
    "    y_train.append(value['label'])\n",
    "for item in train:\n",
    "    item = item.replace('\\n', '')\n",
    "\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#pre-process dev_data and label\n",
    "import json\n",
    "f = open('/Users/enn/Desktop/nlp/project1/project-files/dev.json')\n",
    "dev_data = json.load(f)\n",
    "\n",
    "dev = []\n",
    "y_dev = []\n",
    "\n",
    "for key, value in dev_data.items():\n",
    "    dev.append(value['text'])\n",
    "    y_dev.append(value['label'])\n",
    "for item in dev:\n",
    "    item = item.replace('\\n', '')\n",
    "\n",
    "print(len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n"
     ]
    }
   ],
   "source": [
    "#pre-process test data\n",
    "f = open('/Users/enn/Desktop/nlp/project1/project-files/test-unlabelled.json')\n",
    "test_data = json.load(f)\n",
    "\n",
    "test = []\n",
    "test_title = []\n",
    "for key, value in test_data.items():\n",
    "    test.append(value['text'])\n",
    "    test_title.append(key)\n",
    "for item in test:\n",
    "    item = item.replace('\\n', '')\n",
    "\n",
    "print(len(test_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete symbols in train & dev data\n",
    "import re\n",
    "\n",
    "string = ''\n",
    "for event in train:\n",
    "    string += event\n",
    "for item in dev:\n",
    "    string += item\n",
    "string = re.sub(r'[^\\w\\s]','', string)\n",
    "#print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46598\n",
      "21190\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "#wordtoken & abandon words whose frequency is 1. Count the size of vocabulary\n",
    "c = Counter(string.split())\n",
    "C = sorted(c.most_common(), key=lambda x:x[1])\n",
    "#print(c)\n",
    "print(len(C))\n",
    "stop = []\n",
    "for item in C:\n",
    "    if item[1] == 1:\n",
    "        stop.append(item[0])\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english') + stop)\n",
    "#print(len(stopwords))\n",
    "def preprocess_events(events):\n",
    "    tokenized_events = []\n",
    "    for event in events:\n",
    "        event = re.sub(r'[^\\w\\s]','', event)\n",
    "        all_tokens = [token for token in tt.tokenize(event) if not token in stopwords]\n",
    "        tokenized_events.append(all_tokens)\n",
    "    return tokenized_events\n",
    "#print(len(train))\n",
    "x_train = preprocess_events(train)\n",
    "x_dev = preprocess_events(dev)\n",
    "x_test = preprocess_events(test)\n",
    "#print(len(x_train))\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.4572 - accuracy: 0.7784\n",
      "Epoch 2/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.2297 - accuracy: 0.9224\n",
      "Epoch 3/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.1392 - accuracy: 0.9556\n",
      "Epoch 4/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0788 - accuracy: 0.9756\n",
      "Epoch 5/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0517 - accuracy: 0.9838\n",
      "Epoch 6/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0302 - accuracy: 0.9914\n",
      "Epoch 7/25\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0237 - accuracy: 0.9958\n",
      "Epoch 8/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0104 - accuracy: 0.9978\n",
      "Epoch 9/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0096 - accuracy: 0.9978\n",
      "Epoch 10/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0051 - accuracy: 0.9990\n",
      "Epoch 11/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0059 - accuracy: 0.9988\n",
      "Epoch 12/25\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0188 - accuracy: 0.9960\n",
      "Epoch 13/25\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0134 - accuracy: 0.9970\n",
      "Epoch 14/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0095 - accuracy: 0.9982\n",
      "Epoch 15/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0147 - accuracy: 0.9954\n",
      "Epoch 16/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0085 - accuracy: 0.9976\n",
      "Epoch 17/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0042 - accuracy: 0.9996\n",
      "Epoch 18/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9996\n",
      "Epoch 19/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0055 - accuracy: 0.9982\n",
      "Epoch 20/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 21/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 3.9914e-04 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 2.9085e-04 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 2.1733e-04 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 1.7112e-04 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 1.3430e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = \"<UNK>\")\n",
    "tokenizer.fit_on_texts(train)\n",
    "words = tokenizer.texts_to_matrix(train, mode = \"count\")\n",
    "vocab_size = words.shape[1]\n",
    "embedding_dim = 30\n",
    "maxlen = 30\n",
    "prob = 0.8\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train)\n",
    "test_seq = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "#spilt texts into chunks\n",
    "def chunks(lst, n, minlen):\n",
    "    return [lst[i:i+n] for i in range(0,len(lst),n) if len(lst[i:i+n]) > minlen]\n",
    "\n",
    "minlen = 3\n",
    "pre_train = []\n",
    "ytrain = []\n",
    "for i in range(len(train_seq)):\n",
    "    seq_len = len(train_seq[i])\n",
    "    if seq_len > maxlen:\n",
    "        for chunk in chunks(train_seq[i], int(seq_len/(seq_len/maxlen + 1)), minlen):\n",
    "            pre_train.append(chunk)\n",
    "            ytrain.append(y_train[i])\n",
    "    else:\n",
    "        pre_train.append(train_seq[i])\n",
    "        ytrain.append(y_train[i])\n",
    "        \n",
    "pre_test = []\n",
    "packed_indices = {}\n",
    "count = 0\n",
    "for i in range(len(test_seq)):\n",
    "    seq_len = len(test_seq[i])\n",
    "    indices = []\n",
    "    if seq_len > maxlen:\n",
    "        for chunk in chunks(test_seq[i], int(seq_len/(seq_len/maxlen + 1)), minlen):\n",
    "            pre_test.append(chunk)\n",
    "            indices.append(count)\n",
    "            count += 1\n",
    "        packed_indices[i] = indices\n",
    "    else:\n",
    "        pre_test.append(test_seq[i])\n",
    "        packed_indices[i] = [count]\n",
    "        count += 1\n",
    "\n",
    "x_train = pad_sequences(pre_train, padding = 'post', maxlen = maxlen)\n",
    "x_test = pad_sequences(pre_test, padding = 'post', maxlen = maxlen)\n",
    "Xtrain = np.array(x_train)\n",
    "Ytrain = np.array(ytrain)\n",
    "Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = vocab_size, \n",
    "                           output_dim = embedding_dim, \n",
    "                           input_length = maxlen))\n",
    "model.add(LSTM(10))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "model.fit(Xtrain[:5000], Ytrain[:5000], epochs=25, verbose=True, batch_size=10)\n",
    "y_test = model.predict_classes(x_test)\n",
    "\n",
    "def allocate_class(packed_indices, predict, prob):\n",
    "    predictions = []\n",
    "    for _, indices in packed_indices.items():\n",
    "        if(sum([predict[i] for i in indices])/len(indices) > prob):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "test_result = allocate_class(packed_indices, [y[0] for y in y_test], prob)\n",
    "\n",
    "output_path = '/Users/enn/Desktop/nlp/project1/project-files/test-output.json'\n",
    "output = {}\n",
    "for i in range(len(test_title)):\n",
    "    output[test_title[i]] = {'label': int(test_result[i])}\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
